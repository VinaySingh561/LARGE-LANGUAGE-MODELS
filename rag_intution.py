# -*- coding: utf-8 -*-
"""RAG intution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nEprdqgT97--xP2iJewHiG5D3qk4ednD
"""

!pip install transformers faiss-cpu

documents = [
    "RAG stands for Retrieval-Augmented Generation.",
    "It combines retrieval with large language models.",
    "Vector search finds relevant documents based on embeddings."
]

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
# Load embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')
## encode document
encoding_document = model.encode(documents)

encoding_document.shape

# Create FAISS index
index = faiss.IndexFlatL2(encoding_document.shape[1])
index



print(f"Number of vectors in the index: {index.ntotal}")
print(f"Is the index trained? {index.is_trained}")

index.add(encoding_document)

print(f"Number of vectors in the index: {index.ntotal}")
print(f"Is the index trained? {index.is_trained}")

query = "What does IIT mean?"
query_embedding = model.encode([query])

# Search top 2 docs
_, indices = index.search(query_embedding, k=2)

indices

retrieved = [documents[i] for i in indices[0]]
print(retrieved)

"""# Task
Integrate the provided code snippets to create a system that uses the Hugging Face Inference Client with the "meta-llama/Meta-Llama-3-8B-Instruct" model to answer the question "What is the capital of France?", incorporating the setup for the tokenizer and model (though they won't be directly used in the final inference call).

## Refine retrieved documents

### Subtask:
Format the retrieved documents into a single string to be used as context for the language model.

**Reasoning**:
Join the retrieved documents into a single string with newline separators to create the context for the language model.
"""

context = "\n".join(retrieved)

"""## Create prompt

### Subtask:
Construct a prompt that includes the original query and the refined retrieved documents as context.

**Reasoning**:
Construct the prompt string by combining the original query and the retrieved documents as context, following the instructions for formatting.
"""

prompt = f"""Answer the following question based on the provided context.

Context:
{context}

Question:
{query}
"""

"""## Generate response

### Subtask:
Use the Hugging Face Inference Client to generate a response based on the created prompt.

**Reasoning**:
Use the Hugging Face Inference Client to generate a response based on the created prompt.

**Reasoning**:
The previous cell failed because the `client` object was not defined. This is likely due to the cell where it was defined failing to execute successfully. I will re-execute the cell that defines the `client` object before attempting to use it again.

**Reasoning**:
The previous command failed because the environment variable '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' was not found. This indicates that the API key is not set correctly as an environment variable. I will directly use the API key string in the `InferenceClient` constructor instead of accessing it through `os.environ`.

## Generate response

### Subtask:
Retry using the Hugging Face Inference Client to generate a response based on the created prompt, ensuring the correct API key and provider are used.

**Reasoning**:
Retry using the Hugging Face Inference Client to generate a response based on the created prompt, ensuring the correct API key and provider are used as specified in the instructions.
"""

from huggingface_hub import InferenceClient

client = InferenceClient(
    provider="novita",
    api_key="",
)

completion = client.chat.completions.create(
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    messages=[
        {
            "role": "user",
            "content": prompt
        }
    ],
)

print(completion.choices[0].message)

"""## Print response

### Subtask:
Display the generated response.

**Reasoning**:
Access and print the content of the generated response.
"""

print(completion.choices[0].message.content)

